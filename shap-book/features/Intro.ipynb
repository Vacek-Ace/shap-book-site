{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP (SHapley Additive exPlanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP (SHapley Additive exPlanations) es un enfoque de teoría de juegos para explicar el resultado de cualquier modelo de *machine learning*. Conecta la asignación óptima de beneficios con explicaciones locales utilizando los valores clásicos de Shapley de la teoría de juegos. Pero la teoría de juegos necesita al menos dos cosas: un juego y algunos jugadores. ¿Cómo se aplica esto a la explicabilidad del *machine learning*? Imagina que tenemos un modelo predictivo, entonces:\n",
    "\n",
    "- el \"juego\" está reproduciendo el resultado del modelo,\n",
    "- los \"jugadores\" son las características incluidas en el modelo.\n",
    "\n",
    "Lo que Shapley hace es cuantificar la contribución que cada jugador aporta al juego. Lo que hace SHAP es cuantificar la contribución que cada característica aporta a la predicción hecha por el modelo.\n",
    "Es importante destacar que lo que llamamos \"juego\" se refiere a una sola observación. Un juego: una observación. De hecho, SHAP trata sobre la interpretación local de un modelo de predicción."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mds] *",
   "language": "python",
   "name": "conda-env-mds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
